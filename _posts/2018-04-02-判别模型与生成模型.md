---
layout: post
title: 生成模型和判别模型
categories: 机器学习
comments: false
description: 
keywords: 
mathjax: true
---

### 简介
判别模型Discriminative Model，又可以称为条件模型，或条件概率模型。估计的是条件概率分布，  p(class\|context) 。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知机，决策树，支持向量机等。

生成模型Generative Model，又叫产生式模型。估计的是联合概率分布， p(class,context)= p(class\|context) ∗ p(context) 。由数据学习**联合概率密度分布**P(X,Y)，然后求出条件概率分布P(Y\|X)作为预测的模型，即生成模型：P(Y\|X)= P(X,Y)/ P(X)。

**生成模型**基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y\|X)，再利用它进行分类。然后这个过程还得先求出P(X)。P(X)就是你的训练数据的概率分布，这需要大量的训练样本才能很好的描述样本的概率分布。典型的生成模型有：朴素贝叶斯和隐马尔科夫模型等。

### 生成模型和判别模型的优缺点

在监督学习中，两种方法各有优缺点，适合于不同条件的学习问题。

**   生成方法的特点：**上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y\|X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在**隐变量**时，仍可以用生成方法学习。此时判别方法就不能用。

**   判别方法的特点：**判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y\|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，**反映的是异类数据之间的差异**。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

> 形象的对比：
> 1、学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。
> 2、不去学习每一种语言，你只学习这些语言模型之间的差别，然后再分类

```
生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。
```

### 分类器的设计就是在给定训练数据的基础上估计其概率模型P(Y|X)
监督学习的任务就是从数据中学习一个模型（也叫分类器），应用这一模型，对给定的输入X预测相应的输出Y。这个模型的一般形式为决策函数Y=f(X)或者条件概率分布P(Y\|X)。

一般决策函数Y=f(X)是通过学习算法使你的预测和训练数据之间的误差平方最小化，而贝叶斯告诉我们，虽然它没有显式的运用贝叶斯或者以某种形式计算概率，但它实际上也是在隐含的输出极大似然假设（MAP假设）。也就是说学习器的任务是在所有假设模型有相等的先验概率条件下，输出极大似然假设。【跪拜贝叶斯】

### 其他参考

常见的Generative Model主要有：
– Gaussians, Naive Bayes, Mixtures of multinomials
– Mixtures of Gaussians, Mixtures of experts, HMMs
– Sigmoidal belief networks, Bayesian networks
– Markov random fields

常见的Discriminative Model主要有：
– logistic regression
– SVMs
– traditional neural networks
– Nearest neighbor




