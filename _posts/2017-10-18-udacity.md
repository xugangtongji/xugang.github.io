---
layout: post
title:  udacity google 深度学习课程[1]
categories: 课程学习
comments: false
description: 
keywords: udacity,深度学习
---

![](http://oxyyfe6db.bkt.clouddn.com/hexo/images/kaka.ico) 
# 课程1: 从机器学习到深度学习
### 1.引言
- What is machine learning? using lots of dada to teach computers how to dodthings that human were capable of before.

![](http://oxyyfe6db.bkt.clouddn.com/hexo/images/2017-10-18-11-13-35.jpg)

**Q1: 分类器实现行人检测** 

使用一个二分类器(pedestrain/others),在图片中进行滑窗判断(slider)

![](http://oxyyfe6db.bkt.clouddn.com/hexo/images/2017-10-18-11-17-21.jpg)

**Q2: 分类器实现网站搜索排序？**

分类器的输入：搜索信息和网站信息；输出: 相关性 或 不相关；关键字匹配程度，点击量。


### 2.逻辑回归分类器 logistic classifier
 logistic 回归是针对二分类问题

$$
h_\theta(x) = \frac{1}{1 + \exp ( - {\theta ^T}x)}
$$

学习参数：weights， bias。最小化代价函数：

$$
J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)})\log (1-h_\theta(x^{(i)})) \right]\
$$

### 3.softmax回归多分类
softmax 回归是 logistic 回归的一般形式.softmax将分数转换成概率分布。

$$
p(y^{(i)} = j | x^{(i)} ; \theta) = \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}} }
$$

softmax存在冗余参数集，存在多解，但是不存在局部极值解。
在实际应用中，为了使算法实现更简单清楚，往往保留所有参数 $ (\theta_1, \theta_2,\ldots, \theta_n) $，而不任意地将某一参数设置为 0。但此时我们需要对代价函数做一个改动：加入权重衰减。权重衰减可以解决 softmax 回归的参数冗余所带来的数值问题。
通过添加一个权重衰减项来修改代价函数，这个衰减项会惩罚过大的参数值，现在我们的代价函数变为：

![](http://oxyyfe6db.bkt.clouddn.com/hexo/images/2017-10-18-18-40-54.jpg)
有了这个权重衰减项以后 $(\lambda > 0)$，代价函数就变成了严格的凸函数，这样就可以保证得到唯一的解了。


我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。三个类别是**互斥**的，因此更适于选择softmax回归分类器 。三个类别有**重叠**部分，建立三个独立的 logistic回归二分类器更加合适。
> softmax的python代码
> dir:  C:\Users\Willi\Anaconda2\Lib\site-packages
```python
scores = [3.0, 1.0, 0.2]
import numpy as np
def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    x_exp=np.exp(x)
    sum_exp=sum(x_exp)
    out=x_exp/sum_exp
    return out # TODO: Compute and return softmax(x)
    #return np.exp(x)/np.sum(np.exp(x),axis=0)
print(softmax(scores))
# Plot softmax curves
import matplotlib.pyplot as plt
x = np.arange(-2.0, 6.0, 0.1)
scores = np.vstack([x, np.ones_like(x), 0.2 * np.ones_like(x)])
plt.plot(x, softmax(scores).T, linewidth=2)
plt.show()
```
![](http://oxyyfe6db.bkt.clouddn.com/hexo/images/2017-10-19-10-59-49.jpg)
blue: x=[-2. , -1. ,  0. ,  1. ,  2. ,  3. ,  4. ,  5. ]
green=1; red=0.2
> python指令:  np.vstack([x,y]) 纵向连接数组；  np.hstack([x,y])
横向连接数组；np.ones_like(x) np.zeros_like(x)生成和x一样大小的元素都为0/1的array

对softmax函数的求导，这个属于比较基础的问题。

$$
\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}}
$$
高中就学过的求导规则：
$$
f'(x) = \frac{g'(x)h(x) - g(x)h'(x)}{[h(x)]^2}
$$
分$i = j$，$i \ne j$两种情况讨论：
当$i = j$时：
$$
\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}}= \frac{ e^{a_i}\Sigma - e^{a_i}e^{a_j}}{\Sigma^2}=\frac{e^{a_i}}{\Sigma}\frac{\Sigma - e^{a_j}}{\Sigma}=y_i(1 - y_j)
$$
当$i \ne j$时：
$$
\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}}= \frac{ 0 - e^{a_i}e^{a_j}}{\Sigma^2}=-\frac{e^{a_i}}{\Sigma}\frac{e^{a_j}}{\Sigma}=-y_iy_j
$$
### 4.LossFunction 损失函数
#### 对数似然函数
机器学习里面，对模型的训练都是对Loss function进行优化，在分类问题中，我们一般使用[最大似然估计](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1) (是用来估计一个概率模型的参数的一种方法) 来构造损失函数。对于输入的x，其对应的类标签为t，我们的目标是找到这样的一组参数$\theta$使得$p(t|x)$最大。
在二分类的问题中（伯努利分布），我们有：
$$
p(t|x) = (y)^t(1-y)^{1-t}
$$
其中，y = f(x)是模型预测的概率值，t是样本对应的类标签。

将问题泛化为更一般的情况，多分类问题（多项式分布）：
$$
p(t|x) = \prod_{i=1}^{C}P(t_i|x)^{t_i} = \prod_{i=1}^{C}y_i^{t_i}
$$
由于连乘可能导致最终结果接近0的问题，一般对似然函数取对数的负数，变成最小化对数似然函数。**这不就是是交叉熵损失函数么？**
$$
-log\ p(t|x) = -log \prod_{i=1}^{C}y_i^{t_i} = -\sum_{i = i}^{C} t_{i} log(y_{i})
$$

#### 交叉熵
说交叉熵之前先介绍相对熵，相对熵又称为KL散度（Kullback-Leibler Divergence），用来衡量两个分布之间的距离，记为:
$$
\begin{split}D_{KL}(p||q) &= \sum_{x \in X} p(x) log \frac{p(x)}{q(x)} =\sum_{x \in X}p(x)log \ p(x) - \sum_{x \in X}p(x)log \ q(x)  =-H(p) - \sum_{x \in X}p(x)log\ q(x)\end{split}
$$
假设有两个分布p和q，它们在给定样本集上的交叉熵定义为：
$$
CE(p, q) = -\sum_{x \in X}p(x)log\ q(x) = H(p) + D_{KL}(p||q)
$$
从这里可以看出，交叉熵和相对熵相差了H(p)，而当p（label）已知的时候，H(p)是个常数，所以交叉熵和相对熵在这里是等价的，反映了分布p和q之间的相似程度。
对所有n个样本，C个标签，我们有以下loss function：
$$
L = -\sum_{k = 1}^{n}\sum_{i = 1}^{C}t_{ki} log(y_{ki})
$$
其中$x_i$是样本k属于类别i的概率，$y_{ki}$是模型对样本k预测为属于类别i的概率。
**softmax交叉熵LossFunction求导**
对单个样本来说，$L_{CE}$ 对输入$a_j$的导数为：
$$
\frac{\partial l_{CE}}{\partial a_j} = -\sum_{i = 1}^{C}\frac {\partial t_i log(y_i)}{\partial{a_j}} = -\sum_{i = 1}^{C}t_i \frac {\partial log(y_i)}{\partial{a_j}} = -\sum_{i = 1}^{C}t_i \frac{1}{y_i}\frac{\partial y_i}{\partial a_j}
$$
对于$ \frac{\partial{y_{i}}}{\partial{a_{j}}} $的求导结果已经在softmax部分算出。
当$ i = j $时：$ \frac{\partial{y_{i}}}{\partial{a_{j}}} =y_i(1 - y_j) $
当$ i \ne j $时：$ \frac{\partial{y_{i}}}{\partial{a_{j}}} =-y_iy_j $


> - 关于为什么交叉熵和似然函数是一样的解释：似然函数就是衡量当前这个模型下的预测值与样本值label之间的似然度；交叉熵是直接衡量两个分布相异程度，殊途同归。
> - 交叉熵损失函数还解决了反向传播中，最后一层使用sigmoid函数作为激活函数时梯度消失的问题

#### 逻辑回归与softmax回归损失函数
我们将训练模型参数 $ \theta $，使其能够最小化代价函数 ：
$$
\begin{align}
J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right]
\end{align}
$$
其中$ h_\theta(x) = \frac{1}{1+\exp(-\theta^Tx)} $，样本数为m。

Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 $ k $ 个可能值进行了累加,概率使用softmax表示。当分类数是二的时候，二者是等价的。
```python
x=1000000000
y=0.000001
for i in range(1000000):
    x=x+y
print x
print x-1000000000
```