---
layout: post
title: 深度学习: 迁移学习
categories: 深度学习
comments: false
description: 
sitemap: true
keywords: deep learning
---

## Transfer Learning

> 如果深度学习是圣杯，数据是守门人，那么迁移学习就是大门钥匙。

迁移学习技术的重大收益在于可以对模型进行完善的“通用化”。大型模型往往会与数据过度拟合（Overfit），例如所用数据量远远小于模型权重数量，在面对新的数据时效果可能不如测试时那么好。由于迁移学习可以让模型接受不同类型的数据，因此快速可以习得更出色的模型。

实际上，很少有人从头开始训练整个卷积网络（随机初始化），因为拥有足够大小的数据集是相对罕见的。相反，通常在非常大的数据集（例如ImageNet，其包含具有1000个类别的120万个图像）上预先训练ConvNet，然后使用ConvNet作为感兴趣的任务的初始化或固定特征提取器。三种主要的转学习方案如下：

- **ConvNet作为固定特征提取器**。在ImageNet上预先训练一个ConvNet，删除最后一个完全连接的层（该层的输出是ImageNet等不同任务的1000个等级分数），然后将其余的ConvNet视为新数据集的固定特征提取器。在AlexNet中，这将为包含紧接在分类器之前的隐藏层的激活的每个图像计算4096-D向量。我们将这些功能称为**CNN-codes**。为所有图像提取4096-D代码后，为新数据集训练线性分类器（例如线性SVM或Softmax分类器）。
- **微调ConvNet**。第二种策略是不仅在新数据集上替换和重新训练ConvNet之上的分类器，而且还通过继续反向传播来微调预训练网络的权重。可以微调ConvNet的所有层，或者可以保留一些早期层（由于过度拟合问题）并且仅微调网络的某些更TOP部分。这是因为观察到ConvNet的早期特征包含更多通用特征（例如边缘检测器或颜色检测器）。

- **预训练模型**。由于ConvNets需要2-3周的时间来训练ImageNet上的多个GPU，因此通常会看到人们发布他们的最终ConvNet检查点，以便其他可以使用网络进行微调的人获益。例如，Caffe库有一个model zoo，可以在这里使用共享网络权重。


### 以下是4种主要场景的常用经验法则：
何时如何微调？如何确定应在新数据集上采用哪种类型的迁移学习？但两个最重要的因素是新数据集的大小，以及它与原始数据集的相似性（例如ImageNet类似于图像和类的内容，或者非常不同，例如显微镜图像）。

- 新数据集很小，与原始数据集类似。由于数据很小，因为可能过拟合，微调ConvNet并不是一个好主意。由于数据与原始数据类似，因此，最好的想法可能是在CNN-code 训练线性分类器。
- 新数据集很大，与原始数据集类似。由于我们有更多的数据，我们可以更有信心，我们在整个网络中进行微调，也不会过度训练。
- 新数据集很小但与原始数据集非常不同。由于数据很小，因此最好只训练线性分类器。由于数据集非常不同，因此最好从**网络顶部训练分类器**，其中包含更多数据集特定的功能。
- 新数据集很大，与原始数据集非常不同。由于数据集非常大，我们可以预期我们可以从头开始训练ConvNet或者微调。然而，在实践中，使用来自**预训练模型的权重来初始化**通常仍然是有益的。

学习率。与用于计算新数据集的类别得分的新线性分类器的（随机初始化的）权重相比，通常**使用较小的学习速率**来对微调的ConvNet权重进行微调。这是因为我们期望ConvNet权重相对较好，因此我们不希望太快和太多地扭曲它们（特别是当它们上面的新线性分类器正在通过随机初始化进行训练时）。

### 迁移学习技术目前面临的问题包括：

1. 找到预训练所需的大规模数据集
2. 决定用来预训练的模型
3. 不确定为了训练模型还需要额外准备多少数据
4. 使用预训练模型时难以决定在哪里停止
5. 在预训练模型的基础上，确定模型所需层和参数的数量


### 其他参考文献
[How transferable are features in deep neural networks?] (https://arxiv.org/pdf/1411.1792.pdf)