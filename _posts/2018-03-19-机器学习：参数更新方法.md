---
layout: post
title: 机器学习：模型参数更新与初始化
categories: 
comments: false
description: 
keywords: 
mathjax: true
---
#### 基础知识：偏导数与梯度
直观而言，**偏导数**可以让您了解到，当您略微改动一个变量时，函数会发生多大的变化。梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

$$f(x,y) = e^{2y}sin(x)$$ <br>
$$\frac{\partial f}{\partial x}(x,y) = e^{2y}cos(x)$$<br>
$$\frac{\partial f}{\partial y}(x,y) = 2e^{2y}sin(x)$$<br>

函数的**梯度**是偏导数相对于所有自变量的矢量,也就是函数上升、下降最快的地方。

$$\nabla f(x,y) = (\frac{\partial f}{\partial x}(x,y), \frac{\partial f}{\partial y}(x,y)) = (e^{2y}cos(x), 2e^{2y}sin(x))$$<br>

#### 随机梯度下降
在机器学习中，梯度用于随机梯度下降法(stochastic gradient descent)，这里的随机指的是样本随机。我们的损失函数通常具有很多变量（参数估计），而我们尝试通过跟随函数梯度的负方向来尽量降低损失函数。在梯度下降法中，**批量**指的是用于在单次迭代中计算梯度的样本总数。

![](http://p5iojc2zy.bkt.clouddn.com/_posts/_image/2018-03-19-17-08-37.jpg)

随机梯度下降法：一次抽取一个样本

小批量梯度下降法：每批包含 10-1000 个样本

## 梯度裁剪 (gradient clipping)

在应用[**梯度**](https://developers.google.com/machine-learning/glossary/#gradient)值之前先设置其上限。梯度裁剪有助于确保数值稳定性以及防止[梯度爆炸](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)。

![](http://p5iojc2zy.bkt.clouddn.com/_posts/_image/2018-03-19-19-01-39.jpg)



#### 初始化
- normal
- xavier
