---
layout: post
title: 决策树与随机森林-P1
categories: 机器学习
comments: false
description: 
keywords: 
mathjax: true
---

> 决策树算法起源于E.B.Hunt等人于1966年发表的论文“experiments in Induction”，但真正让决策树成为机器学习主流算法的还是Quinlan（罗斯.昆兰）大神（2011年获得了数据挖掘领域最高奖KDD创新奖），昆兰在1979年提出了ID3算法，掀起了决策树研究的高潮。现在最常用的决策树算法是C4.5是昆兰在1993年提出的。

#### 基本概念
决策树就是一棵树，一颗决策树包含一个根节点、若干个内部结点和若干个叶结点；叶结点对应于预测结果，其他每个结点则对应于一个属性判断（特征判断）；每个结点包含的样本集合根据属性判断的结果被划分到子结点中。决策树实现分类和回归的功能，甚至可以实现结构学习（详见博客的结构随机森林做边缘检测）。

决策树停止生长，生成叶节点的条件：
1. 划分的训练样本纯度为1，无需再分
2. 当前属性集为空，也就是特征使用完毕
3. 到达预定深度，或样本集数量过小，再进行分离的意义不大


![](http://p5iojc2zy.bkt.clouddn.com/_posts/_image/2018-03-25-22-32-17.jpg)

<center>fig1. lable属性集判断流程</center>

![](http://p5iojc2zy.bkt.clouddn.com/_posts/_image/2018-03-25-22-33-41.jpg)

<center>fig2. 数值型属性判断以及决策边界</center>


#### 决策树的优点
- 决策树易于理解和实现.人们在通过解释后都有能力去理解决策树所表达的意义。
- 对于决策树，数据的准备往往是简单.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。
- 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。
- 是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。

#### 其他问题
1. 决策树分裂节点使用的特征属性是有放回的吗？

    